{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcf407b1-23cf-4c62-b8e1-0aaabf51d376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from __future__ import annotations\n",
    "from typing import Dict, Any, Iterable, Optional\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, explode, lit\n",
    "from pyspark.sql.types import StructType\n",
    "logger = get_logger(\"functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53322151-4549-448d-8b2e-10172efc2ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calling logging configuration file\n",
    " %run ./logging_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f90bd62e-44af-4995-8961-2dfeca787762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calling configurations notebook\n",
    "%run ./config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fce0536-88c7-4916-9cae-f12e24811306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- I/O helpers -------------------------------------------------------------\n",
    "\n",
    "def read_json_files(\n",
    "    path: str,\n",
    "    *,\n",
    "    schema: Optional[StructType] = None,\n",
    "    mode: str = \"PERMISSIVE\"\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read JSON files into a Spark DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        DBFS path or mount (e.g., '/mnt/raw/nasa/neo').\n",
    "    schema : StructType, optional\n",
    "        Optional explicit schema for performance and stability.\n",
    "    mode : str\n",
    "        Parse mode: 'PERMISSIVE', 'FAILFAST', or 'DROPMALFORMED'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Parsed DataFrame.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Reading JSON from {path} (mode={mode})\")\n",
    "    df = spark().read.option(\"mode\", mode).json(path, schema=schema)\n",
    "    logger.info(f\"Read {df.count()} rows / {len(df.columns)} columns\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4049cc17-cd1f-4762-83ff-64cb49895f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta(\n",
    "    df: DataFrame,\n",
    "    path: str,\n",
    "    *,\n",
    "    mode: str = \"append\",\n",
    "    merge_schema: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Write a DataFrame to Delta.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame to persist.\n",
    "    path : str\n",
    "        Output Delta path.\n",
    "    mode : str\n",
    "        Save mode: 'append' or 'overwrite'.\n",
    "    merge_schema : bool\n",
    "        Whether to merge schema on write.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    DataQualityError\n",
    "        If df is empty, to prevent writing bad checkpoints.\n",
    "    \"\"\"\n",
    "    if df.rdd.isEmpty():\n",
    "        raise DataQualityError(f\"Refusing to write empty DataFrame to {path}\")\n",
    "    logger.info(f\"Writing Delta to {path} (mode={mode}, merge_schema={merge_schema})\")\n",
    "    (df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(mode)\n",
    "       .option(\"mergeSchema\", str(merge_schema).lower())\n",
    "       .save(path))\n",
    "    logger.info(\"Write complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0b7e0df-c4ef-4a83-b22e-898b79d288af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Transformations (example shapes for NASA NEO) ---------------------------\n",
    "\n",
    "def flatten_neo_feed(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Flatten a NASA NEO 'feed' JSON structure into a row-per-object DataFrame.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This assumes input like:\n",
    "      {'near_earth_objects': {'2020-01-01': [ {...}, {...} ], '2020-01-02': [...] }, ...}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Raw DataFrame loaded from the feed format.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Flattened DataFrame with one row per NEO object and a 'close_approach_date' column.\n",
    "    \"\"\"\n",
    "    logger.info(\"Flattening NEO feed structure\")\n",
    "    # near_earth_objects is a map[date -> array]\n",
    "    neo_map_col = \"near_earth_objects\"\n",
    "    exploded_dates = df.selectExpr(f\"stack(1, {neo_map_col}) as tmp\") if neo_map_col not in df.columns else df.select(neo_map_col)\n",
    "    if neo_map_col in df.columns:\n",
    "        # explode map into (date, array)\n",
    "        exploded = df.selectExpr(\"inline(near_earth_objects) as (close_date, objs)\")\n",
    "    else:\n",
    "        exploded = df  # fallback for already exploded structures\n",
    "\n",
    "    # explode the array of objects\n",
    "    flattened = exploded.select(col(\"close_date\"), explode(col(\"objs\")).alias(\"obj\"))\n",
    "    # Pull common fields to top-level; modify as needed based on your raw schema\n",
    "    cols = [\n",
    "        col(\"close_date\").alias(\"close_approach_date\"),\n",
    "        col(\"obj.id\").cast(\"string\").alias(\"neo_id\"),\n",
    "        col(\"obj.name\").alias(\"name\"),\n",
    "        col(\"obj.is_potentially_hazardous_asteroid\").alias(\"is_hazardous\"),\n",
    "        col(\"obj.nasa_jpl_url\").alias(\"reference_url\"),\n",
    "    ]\n",
    "    # Example nested: close_approach_data[0].relative_velocity.kilometers_per_second\n",
    "    # Keep it defensive for missing arrays\n",
    "    first = col(\"obj.close_approach_data\")[0]\n",
    "    velocity = first[\"relative_velocity\"][\"kilometers_per_second\"].cast(\"double\").alias(\"kps\")\n",
    "    miss_distance_km = first[\"miss_distance\"][\"kilometers\"].cast(\"double\").alias(\"miss_km\")\n",
    "\n",
    "    result = flattened.select(*cols, velocity, miss_distance_km)\n",
    "    logger.info(f\"Flattened to {result.count()} rows\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c65200a-0fcb-48a1-a9e7-d5e8642f49a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def expect_non_empty(df: DataFrame, *, step: str) -> None:\n",
    "    \"\"\"\n",
    "    Assert that a DataFrame is non-empty or raise DataQualityError.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame to validate.\n",
    "    step : str\n",
    "        Step name to include in the error message.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    DataQualityError\n",
    "        If the DataFrame is empty.\n",
    "    \"\"\"\n",
    "    if df.rdd.isEmpty():\n",
    "        raise DataQualityError(f\"Validation failed: '{step}' produced an empty DataFrame.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

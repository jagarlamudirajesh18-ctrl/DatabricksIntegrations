{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcf407b1-23cf-4c62-b8e1-0aaabf51d376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "from __future__ import annotations\n",
    "from typing import Dict, Any, Iterable, Optional\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, explode, lit\n",
    "from pyspark.sql.types import StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94a4f28a-c8ff-4731-9329-7c547529ce45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- ensure exceptions exist, even if configs hasn't run ---\n",
    "try:\n",
    "    ConfigError\n",
    "except NameError:\n",
    "    class ConfigError(Exception): ...\n",
    "try:\n",
    "    DataQualityError\n",
    "except NameError:\n",
    "    class DataQualityError(Exception): ...\n",
    "\n",
    "# --- logger shim (uses 'log' from logging_config if available) ---\n",
    "try:\n",
    "    log\n",
    "except NameError:\n",
    "    import logging\n",
    "    log = logging.getLogger(\"functions\")\n",
    "    if not log.handlers:\n",
    "        log.addHandler(logging.StreamHandler())\n",
    "    log.setLevel(logging.INFO)\n",
    "\n",
    "# CE path helpers\n",
    "def _to_local_dbfs(p: str) -> str:\n",
    "    return \"/dbfs/\" + p.split(\"dbfs:/\", 1)[1].lstrip(\"/\") if p.startswith(\"dbfs:/\") else p\n",
    "\n",
    "def _mkdirs_dbfs_dir(dbfs_dir: str):\n",
    "    try:\n",
    "        dbutils.fs.mkdirs(dbfs_dir)\n",
    "    except Exception as e:\n",
    "        raise ConfigError(f\"Cannot create DBFS directory {dbfs_dir}: {e}\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fce0536-88c7-4916-9cae-f12e24811306",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- I/O helpers -------------------------------------------------------------\n",
    "\n",
    "def read_json_files(path: str, mode: str = \"PERMISSIVE\"):\n",
    "    try:\n",
    "        # Spark read with common-safe options\n",
    "        df = (spark.read\n",
    "              .option(\"multiLine\", \"true\")\n",
    "              .option(\"mode\", mode)           # PERMISSIVE | DROPMALFORMED | FAILFAST\n",
    "              .json(path))\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        # Bubble up as ConfigError so your main's except blocks work\n",
    "        raise ConfigError(f\"Failed to read JSON from {path}: {e}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4049cc17-cd1f-4762-83ff-64cb49895f63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta(\n",
    "    df: DataFrame,\n",
    "    path: str,\n",
    "    *,\n",
    "    mode: str = \"append\",\n",
    "    merge_schema: bool = True\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Write a DataFrame to Delta.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        DataFrame to persist.\n",
    "    path : str\n",
    "        Output Delta path.\n",
    "    mode : str\n",
    "        Save mode: 'append' or 'overwrite'.\n",
    "    merge_schema : bool\n",
    "        Whether to merge schema on write.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    DataQualityError\n",
    "        If df is empty, to prevent writing bad checkpoints.\n",
    "    \"\"\"\n",
    "    if df.rdd.isEmpty():\n",
    "        raise DataQualityError(f\"Refusing to write empty DataFrame to {path}\")\n",
    "    logger.info(f\"Writing Delta to {path} (mode={mode}, merge_schema={merge_schema})\")\n",
    "    (df.write\n",
    "       .format(\"delta\")\n",
    "       .mode(mode)\n",
    "       .option(\"mergeSchema\", str(merge_schema).lower())\n",
    "       .save(path))\n",
    "    logger.info(\"Write complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0b7e0df-c4ef-4a83-b22e-898b79d288af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Transformations (example shapes for NASA NEO) ---------------------------\n",
    "\n",
    "def flatten_neo_feed(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Flatten a NASA NEO 'feed' JSON structure into a row-per-object DataFrame.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This assumes input like:\n",
    "      {'near_earth_objects': {'2020-01-01': [ {...}, {...} ], '2020-01-02': [...] }, ...}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Raw DataFrame loaded from the feed format.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Flattened DataFrame with one row per NEO object and a 'close_approach_date' column.\n",
    "    \"\"\"\n",
    "    logger.info(\"Flattening NEO feed structure\")\n",
    "    # near_earth_objects is a map[date -> array]\n",
    "    neo_map_col = \"near_earth_objects\"\n",
    "    exploded_dates = df.selectExpr(f\"stack(1, {neo_map_col}) as tmp\") if neo_map_col not in df.columns else df.select(neo_map_col)\n",
    "    if neo_map_col in df.columns:\n",
    "        # explode map into (date, array)\n",
    "        exploded = df.selectExpr(\"inline(near_earth_objects) as (close_date, objs)\")\n",
    "    else:\n",
    "        exploded = df  # fallback for already exploded structures\n",
    "\n",
    "    # explode the array of objects\n",
    "    flattened = exploded.select(col(\"close_date\"), explode(col(\"objs\")).alias(\"obj\"))\n",
    "    # Pull common fields to top-level; modify as needed based on your raw schema\n",
    "    cols = [\n",
    "        col(\"close_date\").alias(\"close_approach_date\"),\n",
    "        col(\"obj.id\").cast(\"string\").alias(\"neo_id\"),\n",
    "        col(\"obj.name\").alias(\"name\"),\n",
    "        col(\"obj.is_potentially_hazardous_asteroid\").alias(\"is_hazardous\"),\n",
    "        col(\"obj.nasa_jpl_url\").alias(\"reference_url\"),\n",
    "    ]\n",
    "    # Example nested: close_approach_data[0].relative_velocity.kilometers_per_second\n",
    "    # Keep it defensive for missing arrays\n",
    "    first = col(\"obj.close_approach_data\")[0]\n",
    "    velocity = first[\"relative_velocity\"][\"kilometers_per_second\"].cast(\"double\").alias(\"kps\")\n",
    "    miss_distance_km = first[\"miss_distance\"][\"kilometers\"].cast(\"double\").alias(\"miss_km\")\n",
    "\n",
    "    result = flattened.select(*cols, velocity, miss_distance_km)\n",
    "    logger.info(f\"Flattened to {result.count()} rows\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c65200a-0fcb-48a1-a9e7-d5e8642f49a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# put this in functions (or wherever expect_non_empty lives)\n",
    "\n",
    "def expect_non_empty(df, step: str = \"dataframe\"):\n",
    "    \"\"\"Works for Spark & pandas; raises DataQualityError if empty.\"\"\"\n",
    "    # Detect Spark DF without importing at top-level in case pandas is used\n",
    "    try:\n",
    "        from pyspark.sql import DataFrame as SparkDF  # available on Databricks\n",
    "        is_spark = isinstance(df, SparkDF)\n",
    "    except Exception:\n",
    "        is_spark = False\n",
    "\n",
    "    try:\n",
    "        if is_spark:\n",
    "            # Fast + safe emptiness checks for Spark\n",
    "            # 1) try RDD.isEmpty(); if that fails, 2) use head(1)\n",
    "            try:\n",
    "                empty = df.rdd.isEmpty()\n",
    "            except Exception:\n",
    "                empty = (df.head(1) == [])  # no action if at least one row\n",
    "        else:\n",
    "            # pandas (or pandas-like)\n",
    "            empty = getattr(df, \"empty\", None)\n",
    "            if empty is None:\n",
    "                # last-resort check for sequences\n",
    "                empty = (len(df) == 0)  # only for objects that support len()\n",
    "    except TypeError:\n",
    "        # If len() is not supported (e.g., Spark DF leaked here), fallback to safe Spark head(1)\n",
    "        try:\n",
    "            empty = (df.head(1) == [])\n",
    "        except Exception:\n",
    "            # If we truly can't check, assume not empty to avoid false failures\n",
    "            empty = False\n",
    "\n",
    "    if empty:\n",
    "        raise DataQualityError(f\"{step} is empty\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
